version: '3.8'

services:
  # =====================================================================
  # REDIS (Still needed internally for Airflow Scheduler)
  # =====================================================================
  redis:
    image: redis:7-alpine
    container_name: efiche_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    networks:
      - efiche_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s

  # =====================================================================
  # AIRFLOW CONFIGURATION
  # =====================================================================
  x-airflow-common: &airflow-common
    image: apache/airflow:2.10.2-python3.11
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      
      # CONNECTION STRING: Uses host.docker.internal to reach your running DBs
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_HOST_INTERNAL}:${AIRFLOW_DB_PORT_INTERNAL}/${AIRFLOW_DB_NAME}
      
      # VARIABLES FOR YOUR PYTHON ETL SCRIPTS
      DB_HOST: ${EFICHE_DB_HOST_INTERNAL}
      DB_PORT: ${EFICHE_DB_PORT_INTERNAL}
      DB_NAME: ${PG_DATABASE_OPERATIONAL}
      DB_USER: ${PG_USER}
      DB_PASSWORD: ${PG_PASSWORD}  # <--- Fixes the auth error
      PG_PASSWORD: ${PG_PASSWORD}
     
      
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW_GID: ${AIRFLOW_GID:-50000}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # ETL ARGS
      NUM_ROWS: ${NUM_ROWS:-1000}
      BATCH_SIZE: ${BATCH_SIZE:-1000}
      APPEND_MODE: ${APPEND_MODE:-true}
      HF_TOKEN: ${HF_TOKEN}
      
    networks:
      - efiche_network
    # CRITICAL: This enables access to your external database
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - airflow_home:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./pipeline:/opt/airflow/pipeline
      - ./synthetic_data_engine:/opt/airflow/synthetic_data_engine
      - ./synthetic_data_engine:/opt/airflow/synthetic_data_engine
      - ./utils:/opt/airflow/dags/utils  # <--- ADD THIS LINE

  # =====================================================================
  # AIRFLOW INITIALIZATION
  # =====================================================================
  airflow_init:
    <<: *airflow-common
    container_name: airflow_init
    restart: "no"
    entrypoint: /bin/bash
    command: >
      -c "
      airflow db init &&
      airflow users create
      --username ${AIRFLOW_USER}
      --firstname ${AIRFLOW_FIRSTNAME}
      --lastname ${AIRFLOW_LASTNAME}
      --role Admin
      --email ${AIRFLOW_EMAIL}
      --password ${AIRFLOW_PASSWORD} || true"
    # No depends_on DB, because DB is external

  # =====================================================================
  # AIRFLOW WEBSERVER
  # =====================================================================
  airflow_webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      redis:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully

  # =====================================================================
  # AIRFLOW SCHEDULER
  # =====================================================================
  airflow_scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    command: scheduler
    depends_on:
      redis:
        condition: service_healthy
      airflow_init:
        condition: service_completed_successfully

  # =====================================================================
  # SUPERSET
  # =====================================================================
  superset:
    image: apache/superset:latest
    container_name: efiche_superset
    restart: unless-stopped
    ports:
      - "${SUPERSET_PORT}:${SUPERSET_PORT}"
    env_file:
      - .env
    environment:
      SUPERSET_SECRET_KEY: "change_this_key_in_production"
      SUPERSET_SQLALCHEMY_DATABASE_URI: "${SUPERSET_SQLALCHEMY_DATABASE_URI}"
      SUPERSET_CONFIG_PATH: "/app/superset_home/superset_config.py"
      SQLLAB_TIMEOUT: 300
      PYTHONPATH: "/app/superset_home:/app/superset_home/.local/lib/python3.10/site-packages"
    # Also needs to see the external database
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./superset/superset_home:/app/superset_home
    command: >
      /bin/sh -c "
      pip install psycopg2-binary &&
      superset db upgrade &&
      superset fab create-admin --username $${ADMIN_USER} --firstname Superset --lastname Admin --email $${ADMIN_EMAIL} --password $${ADMIN_PASS} || true &&
      superset init &&
      /usr/bin/run-server.sh"
    networks:
      - efiche_network

networks:
  efiche_network:
    driver: bridge

volumes:
  airflow_home:
